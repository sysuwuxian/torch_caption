require 'nn'
local utils = require 'misc.utils'
local net_utils = require 'misc.net_utils'
local GRU = require 'misc.GRU'
local LSTM = require 'misc.LSTM'
local tree_lstm = require 'misc.tree_lstm'
-------------------------------------------------------------------------------
-- Language Model core
-------------------------------------------------------------------------------

local layer, parent = torch.class('nn.LanguageParsing', 'nn.Module')
function layer:__init(opt)
  parent.__init(self)

  -- options for core network
  self.vocab_size = utils.getopt(opt, 'vocab_size') -- required
  self.input_encoding_size = utils.getopt(opt, 'input_encoding_size')
  self.rnn_size = utils.getopt(opt, 'rnn_size')
  self.num_layers = utils.getopt(opt, 'num_layers', 1)
  local dropout = utils.getopt(opt, 'dropout', 0)
  -- options for Language Model
  self.seq_length = utils.getopt(opt, 'seq_length')
  self.use_lstm = utils.getopt(opt, 'useLSTM')
  self.input_size_img = utils.getopt(opt, 'input_encoding_size')
  self.img_seq_size = utils.getopt(opt, 'att_seq_size')
  
  -- create the core lstm network. note +1 for both the START and END tokens
  
  if opt.useLSTM then
    model = LSTM.lstm_att(self.input_encoding_size, self.input_size_img, self.img_seq_size, 
          self.vocab_size + 1, self.rnn_size, self.num_layers, dropout)
  end
  -- +1 for start tokens
  self.core = nn.Recursor(model)
  self:_creatZeroState(1) -- lazy init

  -- for tree_lstm init
  
  self.tree_lstm = tree_lstm.tree(self.input_size_img, self.input_size_img)
  self.tree_module = nn.Recursor(self.tree_lstm)
end

function layer:_creatZeroState(batch_size)

  assert(batch_size ~= nil, 'batch_size must be provided')
  if not self.zero_state then self.zero_state = {} end -- lazy init

  for h=1, self.use_lstm and 2 * self.num_layers or self.num_layers do
    if self.zero_state[h] then
      if self.zero_state[h]:size(1) ~= batch_size then
        self.zero_state[h]:resize(batch_size, self.rnn_size):zero()
      end
    else 
      --self.zero_state[h] = torch.zeros(batch_size, self.rnn_size)
      self.zero_state[h] = torch.zeros(self.rnn_size)
    end
  end
  self.num_state = #self.zero_state
end

function layer:getModulesList()
  return {self.core, self.tree_module}
end

function layer:parameters()
  -- we only have two internal modules, return their params
  local p1,g1 = self.core:parameters()
  local p2,g2 = self.tree_module:parameters()

  local params = {}
  for k,v in pairs(p1) do table.insert(params, v) end
  for k,v in pairs(p2) do table.insert(params, v) end
  
  local grad_params = {}
  for k,v in pairs(g1) do table.insert(grad_params, v) end
  for k,v in pairs(g2) do table.insert(grad_params, v) end

  return params, grad_params
end

function layer:training()
  self.core:training()
  self.tree_module:training()
end

function layer:evaluate()
  self.core:evaluate()
  self.tree_module:evaluate()
end

--[[
takes a batch of images and runs the model forward in sampling mode
Careful: make sure model is in :evaluate() mode if you're calling this.
Returns: a DxN LongTensor with integer elements 1..M, 
where D is sequence length and N is batch (so columns are sequences)
--]]

-- input is a table
function layer:sample(input)

  self.core:forget()
  self.tree_module:forget()

  local init_state = input[1]
  
  -- seq is T * N
  local trees = input[2]
  
  -- transition is N * transition
  local transitions = input[3]
  
  local configs = input[4]

  local batch_size = #configs
  local att_seq_size = self.img_seq_size
  

  local trans_num = transitions:size(2) 

  local seq = torch.LongTensor(self.seq_length, batch_size):zero() 

  -- consider sent is "I was a boy"
  -- lstm input is "# I was a boy"
  -- lstm output is "I was a boy #" 

  -- enumarate the tree
  
  for i = 1, batch_size do
    local tree = trees[i]
    local config = configs[i]
    local lstm_state = {}

    local cnt = 1
    
    for t = 1, trans_num do
      local state = transitions[i][t]
      if state == 0 then
        config:shift()
        local it, word
        if cnt == 1 then
          word = self.vocab_size + 1
          lstm_state = {init_state[1][i], init_state[2][i]}
        else
           _, tensor = torch.max(logprobs, 1)
           word = tensor[1]
        end
        it = torch.LongTensor(1):fill(word)

        if cnt >= 2 then
          seq[cnt-1][i] = it 
        end
        -- shift -> soft attention to generator the next word
        -- get the corresponding inputs, mask, is_leaf for backward
        
        local inputs = {it, tree.feat, tree.mask, unpack(lstm_state)}

        local out = self.core:forward(inputs)
        logprobs = out[self.num_state + 1]:float()

        lstm_state = {}
        assert(out[1]:dim() == 1 and out[2]:dim() == 1)
        for k = 1, self.num_state do table.insert(lstm_state, out[k]) end

        -- find the relation between word and region
        local att_w = out[self.num_state+2]:float() 
        local _, id = torch.max(att_w, 2)
        -- mapping current word to corresponding region
        tree.word2region[cnt] = id[1][1]
        cnt = cnt + 1
      
      elseif state == 1 then
        -- left arc
        local son, fa = unpack(config:getTop())
        tree:setHead(son, fa, self.tree_module)
        config:removeSecondTopConfig()
      
      elseif state == 2 then
        -- right arc
        local fa, son = unpack(config:getTop())
        tree:setHead(son, fa, self.tree_module)
        config:removeTopConfig()
      else
        break
      end
    end
    --tree:save()
  end
  return seq
end

function layer:updateOutput(input)
  
  self.core:forget()
  self.tree_module:forget()

  local init_state = input[1]
  
  -- seq is T * N
  local seq = input[2]
  local trees = input[3]
  
  -- transition is N * transition
  local transitions = input[4]
  local configs = input[5]

  local att_seq_size = self.img_seq_size
  
  self.inputs = {}
  self.state = {}
  self.tree = {}
  self.merge_node = {}
  self.diff_max = {}
  self.tmax = {}
  self.mask = {}


  local batch_size = seq:size(2)
  local trans_num = transitions:size(2) 

  self.output:resize(self.seq_length+1, batch_size, self.vocab_size+1)


  -- consider sent is "I was a boy"
  -- lstm input is "# I was a boy"
  -- lstm output is "I was a boy #" 

  -- enumarate the tree

  for i = 1, batch_size do
    local tree = trees[i]
    local config = configs[i]

    local cnt = 1
    self.inputs[i] = {}
    self.merge_node[i] = {}
    self.mask[i] = {}
    
    for t = 1, trans_num do
      local state = transitions[i][t]
      if state == 0 then
        -- config operation
        config:shift()
        
        local it, word
        if cnt == 1 then
          word = self.vocab_size + 1
          self.state[0] = {init_state[1][i], init_state[2][i]}
        else
          word = seq[cnt-1][i]
        end
        it = torch.LongTensor(1):fill(word)
        -- shift -> soft attention to generator the next word
        -- get the corresponding inputs, mask, is_leaf for backward
        
        self.inputs[i][cnt] = {it, tree.feat, tree.mask, unpack(self.state[cnt-1])}
        self.mask[i][t] = tree.mask 

        local out = self.core:forward(self.inputs[i][cnt])
        self.output[cnt][i] = out[self.num_state+1]

        self.state[cnt] = {} -- the rest is state
        assert(out[1]:dim() == 1 and out[2]:dim() == 1)
        for k = 1, self.num_state do table.insert(self.state[cnt], out[k]) end

        -- find the relation between word and region
        local att_w = out[self.num_state+2]:float() 
        local _, id = torch.max(att_w, 2)

        -- mapping current word to corresponding region
        tree.word2region[cnt] = id[1][1]
        cnt = cnt + 1

        if utils.is_empty(config.buffer) == true then
          self.diff_max[i] = t
          break
        end
      
      elseif state == 1 then
        -- left arc
        local son, fa = unpack(config:getTop())
        if tree:setHead(son, fa, self.tree_module) == true then
          self.merge_node[i][t] = son
        end
        config:removeSecondTopConfig()
      
      elseif state == 2 then
        -- right arc
        local fa, son = unpack(config:getTop())
        if tree:setHead(son, fa, self.tree_module) == true then
          self.merge_node[i][t] = son
        end
        config:removeTopConfig()
      else
        break
      end
    end
    -- need to subtract by 1
    self.tmax[i] = cnt - 1
    self.tree[i] = tree
  end
  return self.output
end



--[[
gradOutput is an (D+2)xNx(M+1) Tensor.
--]]
function layer:updateGradInput(input, gradOutput)
 
  self:_creatZeroState(1)
  assert(#self.tree >= 1)
  
  local batch_size = input[2]:size(2)

  local att_num = self.tree[1].feat:size(1)

  local dim = self.tree[1].feat:size(2)

  local dimgs = torch.CudaTensor(batch_size, att_num, dim):fill(0)
 
  local dc0 = torch.CudaTensor(batch_size, self.rnn_size):fill(0)
  local dh0 = torch.CudaTensor(batch_size, self.rnn_size):fill(0)

  -- get the trainsition 
  local transition = input[3]

  local trans_num = input[3]:size(1)

  for i = batch_size, 1, -1 do
    local dstate = {[self.tmax[i]] = self.zero_state}
    local cnt = self.tmax[i] 
    local tree = self.tree[i]
    -- verify the node and feat
    local dnode_h = {}
    local dnode_c = {}
    local dfeat = {}

    for t = self.diff_max[i], 1, -1 do
        local trans = transition[i][t] 
        if trans == 1 then
          -- get father
          if self.merge_node[i][t] ~= nil then
            tokenIndex = self.merge_node[i][t]
            
            if dnode_c[tokenIndex] == nil then
              dnode_c[tokenIndex] = torch.CudaTensor(dim):fill(0)
            end
            if dnode_h[tokenIndex] == nil then
              dnode_h[tokenIndex] = torch.CudaTensor(dim):fill(0)
            end

            tree:update_backward(tokenIndex, self.tree_module, dnode_h, dnode_c, dfeat)  
          end
        elseif trans == 2 then
          if self.merge_node[i][t] ~= nil then
            tokenIndex = self.merge_node[i][t]

            if dnode_h[tokenIndex] == nil then
              dnode_h[tokenIndex] = torch.CudaTensor(dim):fill(0)
            end
            if dnode_c[tokenIndex] == nil then
              dnode_c[tokenIndex] = torch.CudaTensor(dim):fill(0)
            end

            tree:update_backward(tokenIndex, self.tree_module, dnode_h, dnode_c, dfeat) 
          end
        elseif trans == 0 then
          local dout = {}
          for k = 1, #dstate[cnt] do table.insert(dout, dstate[cnt][k]) end
          table.insert(dout, gradOutput[cnt][i])
          -- notice we need backward zero Tensor
          local zeroTensor = torch.CudaTensor(att_num):zero()

          table.insert(dout, zeroTensor)
          local dinputs = self.core:backward(self.inputs[cnt], dout)
          dstate[cnt-1] = {}
          for k = 4, self.num_state+3 do table.insert(dstate[cnt-1], dinputs[k]) end

          -- online change the dinput0
          -- enumarte the attention part 
          for k = 1, att_num do
            if self.mask[i][t][k] == 0 then
              if tree:is_origin_region(k) then
                utils.add(dfeat[key], dinputs[2][k])
              else
                local node_id = tree.region2h[k]
                utils.add(dnode_h[node_id], dinputs[2][k])
              end
            end
          end
          cnt = cnt - 1
          if cnt == 0 then
            dc0[i] = dstate[cnt][1]
            dh0[i] = dstate[cnt][2]
          end
        end
    end
    for k = 1, att_num do
      if dfeat[k] ~= nil then
        dimgs[i][k] = dfeat[k]
      end
    end
  end
  self.gradInput = {{dc0, dh0}, dimgs, torch.Tensor(), torch.Tensor()} 
end
